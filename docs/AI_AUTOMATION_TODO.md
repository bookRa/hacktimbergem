# AI Automation Implementation TODO

This document tracks the phased implementation of the AI Automation Strategy outlined in `AI_AUTOMATION_STRATEGY.md`.

## Legend
- ‚úÖ Complete
- üöß In Progress
- ‚è≥ Pending
- ‚è∏Ô∏è Blocked/Deferred

---

## Phase 0: Repository Prep & Instrumentation (1 week)

### 1. Automation Playbooks Workspace ‚úÖ
- ‚úÖ Created `automation_playbooks/` directory structure
- ‚úÖ Implemented `ExperimentConfig` dataclass with YAML support
- ‚úÖ Added `scripts/run_experiment.py` for local/Colab runs
- ‚úÖ Added `scripts/compare_runs.py` for baseline comparison
- ‚úÖ Added `scripts/ingest_colab_bundle.py` for artifact ingestion
- ‚úÖ Created `Makefile` targets (`automation.run`, `automation.compare`)
- ‚úÖ Added `artifacts/` directory with timestamped structure
- ‚úÖ Documented workflow in `automation_playbooks/README.md`

### 2. Observability & Tracing ‚úÖ
- ‚úÖ Created `backend/app/telemetry/` module
- ‚úÖ Implemented `Telemetry` class with LangSmith + OpenTelemetry support
- ‚úÖ Added trace context propagation via `ContextVar`
- ‚úÖ Wrapped `CreateEntityUseCase` with span emission
- ‚úÖ Updated `Settings` with tracing configuration
- ‚úÖ Added `trace_id` to `AIUseCaseResult` for audit trails
- ‚úÖ Integrated telemetry into FastAPI DI container

### 3. AI Use-Case Stubs & Contracts ‚úÖ
- ‚úÖ Implemented `DetectScopesUseCase` with `ScopeSuggestionStrategy` protocol
- ‚úÖ Implemented `DetectSymbolsUseCase` with `SymbolPostProcessor` protocol
- ‚úÖ Implemented `LegendParserUseCase` with `LegendParserStrategy` protocol
- ‚úÖ Created `AIUseCaseResult` container for proposals vs. persisted entities
- ‚úÖ Added `/api/projects/{id}/ai/scopes:detect` endpoint
- ‚úÖ Added `/api/projects/{id}/ai/legends:parse` endpoint
- ‚úÖ Added `/api/projects/{id}/ai/symbols:detect` endpoint
- ‚úÖ Wired AI endpoints into FastAPI with feature flag gating

### 4. Playbook Artifact Integration ‚úÖ
- ‚úÖ Created `PlaybookArtifactLoader` for reading experiment outputs
- ‚úÖ Implemented `PlaybookScopeStrategy` (reads `proposals.json`)
- ‚úÖ Implemented `PlaybookLegendStrategy` (reads `proposals.json`)
- ‚úÖ Implemented `PlaybookVisualService` (reads `detections.json`)
- ‚úÖ Added DI hooks in `dependencies.py` for strategy injection
- ‚úÖ Added `automation_artifacts_dir` to `Settings`

### 5. Domain Model Updates ‚úÖ
- ‚úÖ Confirmed `PageAnchoredEntity` already exists (Legend, Schedule, AssemblyGroup, Note)
- ‚úÖ Confirmed `CreatePageAnchoredEntityBase` supports optional `bounding_box`
- ‚úÖ Updated `CreateEntityUseCase._mark_missing_bbox()` to set validation flags
- ‚úÖ Updated validators to skip bbox normalization when None
- ‚úÖ Extended backend tests for page-anchored entities

### 6. Frontend HITL UX Hooks ‚úÖ
- ‚úÖ Extended `ui_v2.ts` state to track AI proposal context
- ‚úÖ Added "Needs grounding" badge rendering in `EntityTag.tsx`
- ‚úÖ Implemented "Add bounding box" action in `InlineEntityForm.tsx`
- ‚úÖ Added grounding workflow in `OverlayLayer.tsx`
- ‚úÖ Updated `entity_flags.ts` to check for missing bounding boxes
- ‚úÖ Hardened canvas/thumbnail components against null bboxes

### 7. Documentation & Handoff ‚úÖ
- ‚úÖ Created `docs/automation_playbook.md` (Colab‚Üíprod workflow)
- ‚úÖ Documented artifact structure and trace ID propagation
- ‚úÖ Added notebook promotion steps and CLI usage examples
- ‚úÖ Updated `automation_playbooks/README.md` with quick start

### 8. Testing & Quality Gates ‚úÖ
- ‚úÖ Added `backend/tests/test_playbook_strategies.py` (3 tests)
- ‚úÖ Added `backend/tests/test_page_anchored_entities.py` (2 tests)
- ‚úÖ All new tests passing with proper PYTHONPATH setup

---

## Phase 1: Top-Down Scope Extraction (2-3 weeks)

### 1. Data Collection & Annotation ‚è≥
- ‚è≥ Create `backend/tests/data/ai/scopes/` fixture directory
- ‚è≥ Collect 10-15 representative sheet snippets (PNG + OCR JSON)
- ‚è≥ Manually annotate expected scope entities (ground truth)
- ‚è≥ Document annotation guidelines in `docs/ai_data_annotation.md`

### 2. Baseline Heuristic Strategy üöß
- ‚è≥ Implement `HeuristicScopeStrategy` in `backend/app/services/ai/strategies/`
- ‚è≥ Use OCR text patterns (keywords: "SCOPE", "GENERAL NOTES", etc.)
- ‚è≥ Use layout region filtering (top/bottom margins, text density)
- ‚è≥ Add confidence scoring based on pattern matches
- ‚è≥ Wire into `DetectScopesUseCase` via DI

### 3. LLM-Based Scope Strategy ‚è≥
- ‚è≥ Implement `LLMScopeStrategy` using OpenAI/Anthropic API
- ‚è≥ Design prompt template with OCR context + examples
- ‚è≥ Add structured output parsing (JSON schema validation)
- ‚è≥ Implement retry logic with exponential backoff
- ‚è≥ Add cost tracking and rate limiting

### 4. Evaluation Framework ‚è≥
- ‚è≥ Create `backend/tests/test_scope_extraction_eval.py`
- ‚è≥ Implement precision/recall/F1 metrics against ground truth
- ‚è≥ Add regression test suite that runs on CI
- ‚è≥ Document acceptable thresholds in `docs/ai_quality_metrics.md`

### 5. Colab Notebook for Experimentation ‚è≥
- ‚è≥ Create `automation_playbooks/notebooks/scope_extraction.ipynb`
- ‚è≥ Add data loading, model inference, and visualization cells
- ‚è≥ Include artifact export to `artifacts/{timestamp}/` structure
- ‚è≥ Document usage in notebook markdown cells

---

## Phase 2: Bottom-Up Symbol Detection (3-4 weeks)

### 1. Symbol Detection Model Selection ‚è≥
- ‚è≥ Evaluate YOLOv8 vs. Faster R-CNN vs. LayoutLMv3
- ‚è≥ Create training dataset (100+ annotated symbols)
- ‚è≥ Train baseline model on construction drawing symbols
- ‚è≥ Document model architecture in `docs/ai_model_cards.md`

### 2. Visual Detection Service Implementation ‚è≥
- ‚è≥ Implement `YOLOVisualService` or equivalent
- ‚è≥ Add pixel‚ÜíPDF coordinate conversion using `coords.py`
- ‚è≥ Implement confidence filtering and NMS post-processing
- ‚è≥ Add batch processing for multi-sheet projects

### 3. Symbol Clustering & Matching ‚è≥
- ‚è≥ Implement CLIP-based visual similarity clustering
- ‚è≥ Match detected symbols to existing `SymbolDefinition` entities
- ‚è≥ Add fallback to create new definitions for unknown symbols
- ‚è≥ Implement `SymbolPostProcessor` for deduplication

### 4. OCR Integration for Symbol Text ‚è≥
- ‚è≥ Extend detection pipeline to run OCR on symbol bboxes
- ‚è≥ Populate `recognized_text` field for symbol instances
- ‚è≥ Add fuzzy matching to link symbols to schedule items

### 5. Evaluation & Iteration ‚è≥
- ‚è≥ Create annotated test set (50+ sheets with symbols)
- ‚è≥ Measure detection accuracy, false positives, false negatives
- ‚è≥ Iterate on model training and post-processing
- ‚è≥ Document results in `docs/ai_symbol_detection_report.md`

---

## Phase 3: Legend & Schedule Parsing (2-3 weeks)

### 1. Layout Segmentation ‚è≥
- ‚è≥ Implement `LayoutSegmentationService` using LayoutLMv3 or SAM
- ‚è≥ Detect legend/schedule regions on sheets
- ‚è≥ Extract table structure (rows, columns, cells)
- ‚è≥ Add region classification (legend vs. schedule vs. notes)

### 2. Table Parsing & Entity Extraction ‚è≥
- ‚è≥ Implement `TableParserStrategy` for structured data extraction
- ‚è≥ Parse legend items (symbol_text + description)
- ‚è≥ Parse schedule items (mark + description + specifications)
- ‚è≥ Handle multi-line cells and merged cells

### 3. LLM-Based Refinement ‚è≥
- ‚è≥ Use LLM to clean up OCR errors in extracted text
- ‚è≥ Infer missing fields (e.g., schedule_type from title)
- ‚è≥ Add semantic validation (e.g., mark format consistency)

### 4. Integration with Existing Entities ‚è≥
- ‚è≥ Auto-link legend items to symbol definitions
- ‚è≥ Auto-link schedule items to component definitions
- ‚è≥ Populate `defined_in_id` fields automatically

---

## Phase 4: Cross-Modal Linking (2-3 weeks)

### 1. Embedding-Based Similarity ‚è≥
- ‚è≥ Implement `EmbeddingService` using CLIP or sentence-transformers
- ‚è≥ Generate embeddings for text (descriptions, notes)
- ‚è≥ Generate embeddings for visual regions (symbol crops)
- ‚è≥ Add vector store (Milvus, LanceDB, or in-memory FAISS)

### 2. Link Suggestion Strategy ‚è≥
- ‚è≥ Implement `LinkSuggestionStrategy` protocol
- ‚è≥ Score candidate links using spatial + semantic signals
- ‚è≥ Rank suggestions by confidence
- ‚è≥ Filter out low-confidence links (< threshold)

### 3. Spatial Reasoning ‚è≥
- ‚è≥ Implement proximity-based linking (symbols near drawings)
- ‚è≥ Use containment relationships (instances within drawings)
- ‚è≥ Add directional heuristics (callouts pointing to regions)

### 4. LLM-Based Link Validation ‚è≥
- ‚è≥ Use LLM to validate proposed links with context
- ‚è≥ Provide evidence snippets (OCR text, visual crops)
- ‚è≥ Allow human override with feedback loop

---

## Phase 5: HITL Review & Refinement (Ongoing)

### 1. Proposal Review UI ‚è≥
- ‚è≥ Design "AI Proposals" panel in right sidebar
- ‚è≥ Show pending entities/links with confidence scores
- ‚è≥ Add approve/reject/edit actions
- ‚è≥ Implement batch approval workflow

### 2. Feedback Loop ‚è≥
- ‚è≥ Track user corrections (accepted vs. rejected proposals)
- ‚è≥ Store feedback in `artifacts/{timestamp}/feedback.json`
- ‚è≥ Use feedback to retrain models or adjust thresholds
- ‚è≥ Add feedback export for model fine-tuning

### 3. Explainability & Transparency ‚è≥
- ‚è≥ Show evidence for each proposal (OCR snippets, visual crops)
- ‚è≥ Display confidence scores and reasoning
- ‚è≥ Add "Why was this suggested?" tooltip
- ‚è≥ Link to trace IDs for debugging

### 4. Incremental Grounding ‚è≥
- ‚è≥ Implement "Needs grounding" filter in entity list
- ‚è≥ Add bulk grounding workflow (draw multiple bboxes)
- ‚è≥ Allow AI to suggest bboxes for page-scoped entities
- ‚è≥ Track grounding completion metrics

---

## Phase 6: Production Hardening (1-2 weeks)

### 1. Performance Optimization ‚è≥
- ‚è≥ Add caching for OCR/layout results
- ‚è≥ Implement batch processing for multi-sheet projects
- ‚è≥ Optimize coordinate transformations (vectorize)
- ‚è≥ Add progress tracking for long-running AI jobs

### 2. Error Handling & Resilience ‚è≥
- ‚è≥ Add retry logic for API failures (LLM, vision models)
- ‚è≥ Implement graceful degradation (fallback strategies)
- ‚è≥ Add circuit breakers for external services
- ‚è≥ Log errors to observability platform

### 3. Security & Privacy ‚è≥
- ‚è≥ Add API key rotation for external services
- ‚è≥ Implement rate limiting per project/user
- ‚è≥ Sanitize inputs before sending to LLMs
- ‚è≥ Add audit logs for AI-generated entities

### 4. Monitoring & Alerting ‚è≥
- ‚è≥ Set up Grafana dashboards for AI metrics
- ‚è≥ Add alerts for low confidence scores
- ‚è≥ Track cost per project (API usage)
- ‚è≥ Monitor latency and throughput

---

## Infrastructure & Tooling

### Colab Integration ‚è≥
- ‚è≥ Create shared Colab notebook template
- ‚è≥ Add authentication for backend API access
- ‚è≥ Implement artifact upload/download helpers
- ‚è≥ Document GPU setup and dependencies

### CI/CD Pipeline ‚è≥
- ‚è≥ Add regression tests to CI (pytest + fixtures)
- ‚è≥ Run evaluation suite on every PR
- ‚è≥ Block merges if metrics degrade
- ‚è≥ Add smoke tests for AI endpoints

### Model Registry ‚è≥
- ‚è≥ Set up model versioning (MLflow, W&B, or S3)
- ‚è≥ Track model lineage (training data, hyperparameters)
- ‚è≥ Add model deployment workflow
- ‚è≥ Document model promotion criteria

---

## Documentation Needs

### Developer Guides ‚è≥
- ‚è≥ `docs/AI_AUTOMATION_DEVELOPER_GUIDE.md` (architecture overview)
- ‚è≥ `docs/ai_data_annotation.md` (annotation guidelines)
- ‚è≥ `docs/ai_model_cards.md` (model specifications)
- ‚è≥ `docs/ai_quality_metrics.md` (evaluation criteria)

### User Documentation ‚è≥
- ‚è≥ Update `PRD.md` with AI features
- ‚è≥ Add "AI Automation" section to user guide
- ‚è≥ Create video walkthrough of HITL workflow
- ‚è≥ Document best practices for reviewing proposals

---

## Current Status Summary

**Completed:** Phase 0 (Repository Prep & Instrumentation) - 100% ‚úÖ

**Next Steps:**
1. Begin Phase 1 data collection (scope extraction fixtures)
2. Implement baseline heuristic scope strategy
3. Create first Colab notebook for experimentation
4. Draft developer guide structure

**Blockers:** None currently

**Risks:**
- Model accuracy may require multiple iterations
- LLM costs could be higher than expected
- Annotation effort may be underestimated

---

## Notes

- All AI endpoints are feature-flagged via `TIMBERGEM_AI_PLAYBOOKS_ENABLED`
- Trace IDs propagate from Colab ‚Üí artifacts ‚Üí backend for full auditability
- Page-anchored entities can be created without bboxes and grounded later
- Frontend HITL UX is ready to surface AI proposals (pending backend integration)

**Last Updated:** 2025-11-22

